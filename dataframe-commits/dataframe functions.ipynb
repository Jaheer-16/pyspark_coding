{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32a90dad-fb09-441b-8eb4-5ad637ae3bd8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "createOrReplaceTempView"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import*\n",
    "data = [\n",
    "    (1, \"John\", \"Sales\", 50000),\n",
    "    (2, \"Sarah\", \"Engineering\", 75000),\n",
    "    (3, \"Michael\", \"HR\", 60000),\n",
    "    (4, \"Anna\", \"Engineering\", 80000),\n",
    "    (5, \"David\", \"Sales\", 55000)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()\n",
    "\n",
    "df1=df.createOrReplaceTempView(\"employee\")\n",
    "df1=spark.sql(\"Select * from employee where name='John' or department='Engineering'\")\n",
    "df1.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc593216-50a8-430e-bd86-62892cecaaf1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "cast"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import*\n",
    "\n",
    "data = [\n",
    "    (1, \"John\", \"29\", \"50000\", \"2020-01-15\", \"true\"),\n",
    "    (2, \"Sarah\", \"31\", \"75000.50\", \"2019-07-23\", \"false\"),\n",
    "    (3, \"Michael\", \"35\", \"60000\", \"2021-03-10\", \"true\"),\n",
    "    (4, \"Anna\", \"28\", \"80000\", \"2020-11-05\", \"true\"),\n",
    "    (5, \"David\", \"30\", \"55000.75\", \"2018-09-17\", \"false\")\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"salary\", \"joining_date\", \"is_active\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()\n",
    "\n",
    "df.select(col('id').cast('long'))\n",
    "df=df.withColumn('joining_date',col('joining_date').cast('date'))\n",
    "df.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0593017-8211-4758-934e-d1943d9728fa",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771218089626}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "substr"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, substring\n",
    "\n",
    "\n",
    "data = [(1, \"John Doe\"), (2, \"Sarah Connor\"), (3, \"Michael Smith\")]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "\n",
    "df=df.select('id','name',substring(col('name'),1,4).alias('short_name'))\n",
    "df=df.withColumn('last_name',substring(col('name'),5,10))\n",
    "#df=df.select(col(\"last_name\").substr(1,4))\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c81f9d-c46a-4dcc-9941-8d099b29c157",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771220234566}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "when"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "data = [\n",
    "    (1, \"John\", 50000),\n",
    "    (2, \"Sarah\", 75000),\n",
    "    (3, \"Michael\", 60000),\n",
    "    (4, \"Anna\", 80000),\n",
    "    (5, \"David\", 55000)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df1=df.withColumn('salary_level',\n",
    "              when(col(\"salary\") > 75000,'high')\\\n",
    "              .when(col(\"salary\") > 55000,'medium')\\\n",
    "              .otherwise('low'))\n",
    "\n",
    "df1.withColumnRenamed('salary_level','salary_level_new').display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "796e3da2-6c25-4d68-a891-4f10fbca0500",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "filter"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"John\", \"Sales\", 50000),\n",
    "    (2, \"Sarah\", \"Engineering\", 75000),\n",
    "    (3, \"Michael\", \"HR\", 60000),\n",
    "    (4, \"Anna\", \"Engineering\", 80000),\n",
    "    (5, \"David\", \"Sales\", 55000),\n",
    "    (6, \"Emma\", \"HR\", 45000),\n",
    "    (7, \"Chris\", \"Engineering\", 72000)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()\n",
    "df.filter(df.id.isNotNull()).display()\n",
    "df.filter(df.department.contains(\"Eng\")).display()\n",
    "\n",
    "\n",
    "#df.filter((col(\"department\") == \"Engineering\") & (col(\"salary\") > 70000)).display()\n",
    "#df.filter(df.salary > 70000).display()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fc4922e-4dbc-47e6-baec-a0b1419077f2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dropduplicates"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"John\", \"Sales\", 50000),\n",
    "    (2, \"Sarah\", \"Engineering\", 75000),\n",
    "    (3, \"Michael\", \"HR\", 60000),\n",
    "    (4, \"Anna\", \"Engineering\", 80000),\n",
    "    (5, \"David\", \"Sales\", 55000),\n",
    "    (6, \"John\", \"Sales\", 50000),     # duplicate of row 1\n",
    "    (7, \"Sarah\", \"Engineering\", 75000), # duplicate of row 2\n",
    "    (8, \"David\", \"Sales\", 55000)     # duplicate of row 5\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df1=df.dropDuplicates([\"department\"])\n",
    "df1.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d365e81-3f08-4e16-838e-5111e35dd500",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "sort"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"John\", \"Sales\", 50000),\n",
    "    (2, \"Sarah\", \"Engineering\", 75000),\n",
    "    (3, \"Michael\", \"HR\", 60000),\n",
    "    (4, \"Anna\", \"Engineering\", 80000),\n",
    "    (5, \"David\", \"Sales\", 55000)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "#df.sort(df.salary.desc()).display()\n",
    "df.orderBy(col('salary')).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c8969e-ac17-4df3-96bf-b326347ab811",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771223663821}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "groupby"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import*\n",
    "\n",
    "data = [\n",
    "    (1, \"John\", \"Sales\", 50000),\n",
    "    (2, \"Sarah\", \"Engineering\", 75000),\n",
    "    (3, \"Michael\", \"HR\", 60000),\n",
    "    (4, \"Anna\", \"Engineering\", 80000),\n",
    "    (5, \"David\", \"Sales\", 55000),\n",
    "    (6, \"Emma\", \"HR\", 45000),\n",
    "    (7, \"Chris\", \"Engineering\", 72000)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()\n",
    "df.printSchema()\n",
    "#df2 =df.groupBy('department').count()\n",
    "df2=df.groupBy('department').agg(sum('salary').alias('dept_sal'))\n",
    "df3=df.groupBy('department').agg(sum('salary').alias('dept_sal'),avg('salary').alias('avg_sal'))\n",
    "df3.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dataframe functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
